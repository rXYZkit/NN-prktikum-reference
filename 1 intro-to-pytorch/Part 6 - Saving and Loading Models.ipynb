{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving and Loading Models -- 保存和加载模型\n",
    "\n",
    "在本笔记本中，我将向您展示如何使用PyTorch保存和加载模型。 这很重要，因为有时候我们想要先训练一个模型然后再come back加载先前训练过的模型(trained models)以用于进行预测或继续训练新数据(continue training on new data)，在本科我们学习：\n",
    "+ how to save the train models \n",
    "+ how to load them (回来使用这个模型预测)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (一)调用所需模块\n",
    "+ `import fc_model` 这里系统内置(编写了)一个名为 `fc_model`的全连接我分类模型(扩展，如何在jupyter中调用自己编写的模块)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina' \n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import helper\n",
    "import fc_model   #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (二)加载数据集(dataset)\n",
    "+ F_MNIST_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                transforms.Normalize((0.5,), (0.5,))])\n",
    "# Download and load the training data\n",
    "trainset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.FashionMNIST('~/.pytorch/F_MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see one of the images.\n",
    "+ 作为示例，查看其中一幅图片：28x28 gray_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 231,
       "width": 231
      },
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "image, label = next(iter(trainloader))\n",
    "print(image.shape)\n",
    "helper.imshow(image[0,:]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (三)训练神经网络\n",
    "在导入所需库，并加载数据集之后，我们可以开始训练数据集了。(为了使内容更简洁，课程导入了一个名为`fc_model`的模型架构和训练代码存放在名为fc_model的文件中)\n",
    "+ 导入后，我们可以轻松地使用 `fc_model.Network` 创建一个全连接的网络，并使用`fc_model.train`训练这个网络\n",
    "+ 然后使用这个模型(训练之后)来演示如何保存和加载模型   \n",
    "\n",
    "```python\n",
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "```   \n",
    "+ [查看fc_model的源代码] 我们创建一个全连接神经网络模型，它的结构是：784 input units, 10 output units\n",
    "  + 第三个参数`[]`表示隐藏层：3个隐藏层器节点分别为 512， 256， 128\n",
    "  + 输出层由 `logsoft_max` 激活，这样就能使用**负对数似然损失** `nn.NLLLoss`([documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.NLLLoss))\n",
    "  + 最后我们使用 Adam optimizer 来训练我们的神经网络及更新参数   \n",
    "  \n",
    "调用`fc_model`的训练函数   \n",
    "```python\n",
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epoch=2)\n",
    "```   \n",
    "+ 并不知道内部结构，根据输出可以推测是对每一个batch都打印输出！(1个epoch打印23个输出？)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the network, define the criterion and optimizer\n",
    "\n",
    "model = fc_model.Network(784, 10, [512, 256, 128])\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2..  Training Loss: 1.750..  Test Loss: 0.957..  Test Accuracy: 0.683\n",
      "Epoch: 1/2..  Training Loss: 0.985..  Test Loss: 0.735..  Test Accuracy: 0.706\n",
      "Epoch: 1/2..  Training Loss: 0.859..  Test Loss: 0.681..  Test Accuracy: 0.737\n",
      "Epoch: 1/2..  Training Loss: 0.848..  Test Loss: 0.661..  Test Accuracy: 0.749\n",
      "Epoch: 1/2..  Training Loss: 0.701..  Test Loss: 0.640..  Test Accuracy: 0.760\n",
      "Epoch: 1/2..  Training Loss: 0.712..  Test Loss: 0.607..  Test Accuracy: 0.771\n",
      "Epoch: 1/2..  Training Loss: 0.637..  Test Loss: 0.601..  Test Accuracy: 0.780\n",
      "Epoch: 1/2..  Training Loss: 0.697..  Test Loss: 0.557..  Test Accuracy: 0.790\n",
      "Epoch: 1/2..  Training Loss: 0.658..  Test Loss: 0.558..  Test Accuracy: 0.794\n",
      "Epoch: 1/2..  Training Loss: 0.666..  Test Loss: 0.537..  Test Accuracy: 0.806\n",
      "Epoch: 1/2..  Training Loss: 0.656..  Test Loss: 0.536..  Test Accuracy: 0.804\n",
      "Epoch: 1/2..  Training Loss: 0.624..  Test Loss: 0.529..  Test Accuracy: 0.809\n",
      "Epoch: 1/2..  Training Loss: 0.617..  Test Loss: 0.534..  Test Accuracy: 0.808\n",
      "Epoch: 1/2..  Training Loss: 0.591..  Test Loss: 0.532..  Test Accuracy: 0.807\n",
      "Epoch: 1/2..  Training Loss: 0.577..  Test Loss: 0.511..  Test Accuracy: 0.807\n",
      "Epoch: 1/2..  Training Loss: 0.632..  Test Loss: 0.494..  Test Accuracy: 0.820\n",
      "Epoch: 1/2..  Training Loss: 0.593..  Test Loss: 0.506..  Test Accuracy: 0.811\n",
      "Epoch: 1/2..  Training Loss: 0.619..  Test Loss: 0.495..  Test Accuracy: 0.824\n",
      "Epoch: 1/2..  Training Loss: 0.573..  Test Loss: 0.485..  Test Accuracy: 0.825\n",
      "Epoch: 1/2..  Training Loss: 0.605..  Test Loss: 0.487..  Test Accuracy: 0.823\n",
      "Epoch: 1/2..  Training Loss: 0.562..  Test Loss: 0.483..  Test Accuracy: 0.822\n",
      "Epoch: 1/2..  Training Loss: 0.591..  Test Loss: 0.481..  Test Accuracy: 0.819\n",
      "Epoch: 1/2..  Training Loss: 0.581..  Test Loss: 0.477..  Test Accuracy: 0.826\n",
      "Epoch: 2/2..  Training Loss: 0.548..  Test Loss: 0.468..  Test Accuracy: 0.832\n",
      "Epoch: 2/2..  Training Loss: 0.599..  Test Loss: 0.474..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.521..  Test Loss: 0.472..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.560..  Test Loss: 0.488..  Test Accuracy: 0.822\n",
      "Epoch: 2/2..  Training Loss: 0.575..  Test Loss: 0.484..  Test Accuracy: 0.816\n",
      "Epoch: 2/2..  Training Loss: 0.542..  Test Loss: 0.450..  Test Accuracy: 0.833\n",
      "Epoch: 2/2..  Training Loss: 0.511..  Test Loss: 0.461..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.532..  Test Loss: 0.461..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.530..  Test Loss: 0.484..  Test Accuracy: 0.825\n",
      "Epoch: 2/2..  Training Loss: 0.564..  Test Loss: 0.460..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.530..  Test Loss: 0.465..  Test Accuracy: 0.830\n",
      "Epoch: 2/2..  Training Loss: 0.557..  Test Loss: 0.452..  Test Accuracy: 0.840\n",
      "Epoch: 2/2..  Training Loss: 0.514..  Test Loss: 0.457..  Test Accuracy: 0.829\n",
      "Epoch: 2/2..  Training Loss: 0.497..  Test Loss: 0.448..  Test Accuracy: 0.841\n",
      "Epoch: 2/2..  Training Loss: 0.516..  Test Loss: 0.468..  Test Accuracy: 0.831\n",
      "Epoch: 2/2..  Training Loss: 0.518..  Test Loss: 0.441..  Test Accuracy: 0.840\n",
      "Epoch: 2/2..  Training Loss: 0.571..  Test Loss: 0.458..  Test Accuracy: 0.841\n",
      "Epoch: 2/2..  Training Loss: 0.503..  Test Loss: 0.439..  Test Accuracy: 0.840\n",
      "Epoch: 2/2..  Training Loss: 0.506..  Test Loss: 0.434..  Test Accuracy: 0.839\n",
      "Epoch: 2/2..  Training Loss: 0.546..  Test Loss: 0.451..  Test Accuracy: 0.836\n",
      "Epoch: 2/2..  Training Loss: 0.512..  Test Loss: 0.452..  Test Accuracy: 0.838\n",
      "Epoch: 2/2..  Training Loss: 0.486..  Test Loss: 0.447..  Test Accuracy: 0.844\n",
      "Epoch: 2/2..  Training Loss: 0.514..  Test Loss: 0.442..  Test Accuracy: 0.840\n"
     ]
    }
   ],
   "source": [
    "fc_model.train(model, trainloader, testloader, criterion, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 现在我们有了一个已经训练过的神经网络，接下来让我么学习如何**保存及再加载训练过的模型**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (四)保存训练过的神经网络\n",
    "可以想象，如果每次都在需要使用网络时才去训练网络都是不切实际的，我们可以保存经过训练的网络，然后再加载它们以进行更多训练或将其用于预测。\n",
    "+ save it to a file：我们保存模型的方式是实际上保存一个字典 `state_dict`\n",
    "  + 我们的模型的所有参数存储在模型的 `state_dict` 中，这是一个字典文件。 \n",
    "  + 所有参数是指：模型每一层的权重和偏差矩阵\n",
    "  \n",
    "我们先来看看刚刚训练过的**模型的结构**，以及模型的**状态字典(`state_dict`)**的键"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our model: \n",
      "\n",
      " Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ") \n",
      "\n",
      "The state dict keys: \n",
      "\n",
      " odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "print(\"Our model: \\n\\n\", model, '\\n')\n",
    "print(\"The state dict keys: \\n\\n\", model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过这个状态字典的键我们可以了解当保存模型时实际上是保存了什么，这也和 `nn.Linear()` 函数有关，当我们使用`nn.Linear()`创建隐藏层的时候，同时它会字典创建权重矩阵和bias，而这些就是被保存的参数。\n",
    "+ we can save those to a file and load them back to rebuild our model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch 语法1 -- 保存神经网络参数1\n",
    "\n",
    "+ 最简单的事情使用 `torch.save(<>, <>)` 保存模型的参数。例如，我们可以将它保存到文件 `checkpoint.pth` 中\n",
    "  + 执行这个语句之后，会在当前工作目录下生成一个 `checkpoint.pth` 文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch 语法2 -- 加载神经网络参数1\n",
    "+ 读取模型参数字典 `torch.load(<>)`\n",
    "+ 向模型导入参数 `model.load_state_dict(state_dict)`\n",
    "  + 若成功会返回 `<All keys matched successfully>`\n",
    "  + 注意，正确加载网络参数需要model的结构与参数数量相匹配！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['hidden_layers.0.weight', 'hidden_layers.0.bias', 'hidden_layers.1.weight', 'hidden_layers.1.bias', 'hidden_layers.2.weight', 'hidden_layers.2.bias', 'output.weight', 'output.bias'])\n"
     ]
    }
   ],
   "source": [
    "state_dict = torch.load('checkpoint.pth')\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "似乎很简单，但通常还是复杂的，因为只有当 **model architecture** 与 **checkpoint architecture** 完全相同时，才能正确加载 `state_dict`,  如果创建的model具有不同的 architecture，则会报错：\n",
    "+ 但我们在刚刚save网络的状态字典时并没有save网络的结构信息，因此若没有网络结构只有网络字典我们还是不能 rebuild 这个网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100]).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-d859c59ebec0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfc_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mNetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# This will throw an error because the tensor sizes are wrong!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m    828\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m--> 830\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m    831\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for Network:\n\tsize mismatch for hidden_layers.0.weight: copying a param with shape torch.Size([512, 784]) from checkpoint, the shape in current model is torch.Size([400, 784]).\n\tsize mismatch for hidden_layers.0.bias: copying a param with shape torch.Size([512]) from checkpoint, the shape in current model is torch.Size([400]).\n\tsize mismatch for hidden_layers.1.weight: copying a param with shape torch.Size([256, 512]) from checkpoint, the shape in current model is torch.Size([200, 400]).\n\tsize mismatch for hidden_layers.1.bias: copying a param with shape torch.Size([256]) from checkpoint, the shape in current model is torch.Size([200]).\n\tsize mismatch for hidden_layers.2.weight: copying a param with shape torch.Size([128, 256]) from checkpoint, the shape in current model is torch.Size([100, 200]).\n\tsize mismatch for hidden_layers.2.bias: copying a param with shape torch.Size([128]) from checkpoint, the shape in current model is torch.Size([100]).\n\tsize mismatch for output.weight: copying a param with shape torch.Size([10, 128]) from checkpoint, the shape in current model is torch.Size([10, 100])."
     ]
    }
   ],
   "source": [
    "# Try this\n",
    "model = fc_model.Network(784, 10, [400, 200, 100])\n",
    "# This will throw an error because the tensor sizes are wrong!\n",
    "model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch 语法3 -- 保存神经网络参数2\n",
    "\n",
    "因此，实际上为了完全按照训练时的模型重建模型，有关 model architecture 的信息需要与 state dict 一起保存在checkpoint文件中。 为此，您需要构建一个字典，其中包含您需要完全重建模型的所有信息。\n",
    "\n",
    "记得保存模型参数的语句是 `torch.save(<>, <>)`：\n",
    "+ 第一个参数是一个字典文件，若为 `model.state_dict()` 就是模型参数字典，我们需要重新构建一个同时包含model结构信息和参数信息的字典，例如命名为`checkpoint`\n",
    "  + 网络结构包含三个部分：input_size， output_size， hidden_layers(size)一个列表文件\n",
    "  + 另外加上模型参数字典： `model.state_dict()`\n",
    "+ 第二个参数是保存目录，例如当前工作目录"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'input_size': 784,\n",
    "              'output_size': 10,\n",
    "              'hidden_layers': [each.out_features for each in model.hidden_layers],\n",
    "              'state_dict': model.state_dict()}\n",
    "\n",
    "torch.save(checkpoint, 'checkpoint.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  `each.out_features`\n",
    "首先 each 代表的每一个隐藏层对象，即 `nn.Linear()` 所创建的，`nn.Linear()` 是用于设置网络中的全连接层的，需要注意的是全连接层的输入与输出都是二维张量，一般形状为[batch_size, size]，不同于卷积层要求输入输出是四维张量。让我们来看看PyTorch中的`nn.Linaear()`\n",
    "\n",
    "```python\n",
    "torch.nn.Linear(in_features: int, out_features: int, bias: bool = True)\n",
    "```   \n",
    "\n",
    " + `in_features` – size of each input sample\n",
    "\n",
    " + `out_features` – size of each output sample\n",
    " \n",
    " + `bias` – If set to False, the layer will not learn an additive bias. Default: True\n",
    "\n",
    "这里的关键在于使用`nn.Linear()`的对象实例调用参数 `each.out_features`得到隐藏层的units数目，例如第一层隐藏层的`in_features`是输入层的个数，`out_feature`是第一隐藏层的单元数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pytorch 语法4 -- 加载神经网络参数2\n",
    "\n",
    "现在，保存的checkpoint文件具有所有必要的信息来重建训练后的模型。根据需要轻松地将(加载模型参数到模型中)设置为函数：\n",
    "+ 读取字典 `checkpoint`\n",
    "+ 使用`fc_model.Network(<>, <>, <[]>)` 创建一个全连接的网络\n",
    "+ 使用 `model.load_state_dict()` 加载神经网络参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_checkpoint(filepath):\n",
    "    checkpoint = torch.load(filepath)\n",
    "    model = fc_model.Network(checkpoint['input_size'],\n",
    "                             checkpoint['output_size'],\n",
    "                             checkpoint['hidden_layers'])\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 注意这个加载模型的函数必须根据实际情况编写！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (hidden_layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=400, bias=True)\n",
      "    (1): Linear(in_features=400, out_features=200, bias=True)\n",
      "    (2): Linear(in_features=200, out_features=100, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=100, out_features=10, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = load_checkpoint('checkpoint.pth')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们创建了一个model，训练了这个model，保存了这个model，重载了参数，读取/创建了一个新的model，而这个新的model和原理保存的那个一样！"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
